{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f559a28b-5fe3-480b-9fb5-ac6d7fb5c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np \n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35c81cb4-ff55-4169-be2c-1a28b43f5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read surprisal and audibility and clean\n",
    "\n",
    "surprisal_new = pd.read_csv(\"./trf_input/word_surprisal_gpt_target.csv\")\n",
    "\n",
    "surprisal_new['words'] = surprisal_new['words'].str.replace(\",\", \"\")\n",
    "surprisal_new['words'] = surprisal_new['words'].str.replace(\".\", \"\")\n",
    "surprisal_new['words'] = surprisal_new['words'].str.replace(\":\", \"\")\n",
    "surprisal_new['words'] = surprisal_new['words'].str.replace('\"', \"\")\n",
    "surprisal_new['words'] = surprisal_new['words'].str.replace('!', \"\")\n",
    "\n",
    "audibility = pd.read_csv(\"./trf_input/audibility_words_final.csv\")\n",
    "\n",
    "audibility['word'] = audibility['word'].str.replace(\",\", \"\")\n",
    "audibility['word'] = audibility['word'].str.replace(\".\", \"\")\n",
    "audibility['word'] = audibility['word'].str.replace(\":\", \"\")\n",
    "audibility['word'] = audibility['word'].str.replace('\"', \"\")\n",
    "audibility['word'] = audibility['word'].str.replace('!', \"\")\n",
    "\n",
    "\n",
    "surprisal_phone = pd.read_csv(\"./trf_input/phone_surprisal_entropy_final.csv\")\n",
    "\n",
    "word_frequency = pd.read_csv(\"./trf_input/word_frequency_target_lgsubtlex.csv\")\n",
    "word_frequency['frequency']=np.where(word_frequency['word_order'] < 2, np.nan, word_frequency['frequency'])\n",
    "\n",
    "\n",
    "surprisal_new['surprisal']=np.where(surprisal_new['word_order'] < 2, np.nan, surprisal_new['surprisal'])\n",
    "surprisal_new['entropy']=np.where(surprisal_new['word_order'] < 2, np.nan, surprisal_new['entropy'])\n",
    "\n",
    "\n",
    "#Phone audibility (only for timing info)\n",
    "audibility_phone = pd.read_csv(\"./trf_input/audibility_phones_final.csv\")\n",
    "\n",
    "audibility_phone['words'] = audibility_phone['words'].str.replace(\",\", \"\")\n",
    "audibility_phone['words'] = audibility_phone['words'].str.replace(\".\", \"\")\n",
    "audibility_phone['words'] = audibility_phone['words'].str.replace(\":\", \"\")\n",
    "audibility_phone['words'] = audibility_phone['words'].str.replace('\"', \"\")\n",
    "audibility_phone['words'] = audibility_phone['words'].str.replace('!', \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "436e3f08-d9b6-4828-9799-82e44c6d10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load subject responses\n",
    "sent = []\n",
    "names = []\n",
    "part = []\n",
    "sub = []\n",
    "\n",
    "outpath = \"/data/pt_02917/transcriptions_final/\"\n",
    "\n",
    "for path, subdirs, files in os.walk(outpath):\n",
    "    for name in files:\n",
    "        if name.endswith(\".lab\") and name.startswith(\"sub\") or name.startswith(\"41\") or name.startswith(\"67\"):\n",
    "\n",
    "                #load all audios\n",
    "            infile = os.path.join(path, name)\n",
    "\n",
    "            f = codecs.open(infile, \"r\", \"utf-8-sig\")\n",
    "            lines = f.read()\n",
    "\n",
    "            lines = lines.replace(\"ï»¿ \", \"\")\n",
    "            if name.startswith(\"41\"):\n",
    "                sub.append(\"sub41\")\n",
    "            elif name.startswith(\"67\"):\n",
    "                sub.append(\"sub67\")\n",
    "            else:\n",
    "                sub.append(name[0:5])\n",
    "\n",
    "\n",
    "            sent.append(lines)\n",
    "            names.append(name)\n",
    "\n",
    "sub_result = pd.DataFrame(zip(names, sent), columns = ['name', 'resp'])           \n",
    "sub_result['target_id'] = sub_result['name'].str[-7:-4]\n",
    "sub_result['subject'] = sub\n",
    "\n",
    "sub_result['target_id'] = pd.to_numeric(sub_result['target_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1573d294-7d74-4e45-bb84-dc1fa14a6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load subject experiment files\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "##For all pilot subjects\n",
    "subs = ['sub05', 'sub06', 'sub07', 'sub08', 'sub09', 'sub10', 'sub11', 'sub12', 'sub13', 'sub14', 'sub15', 'sub16', \n",
    "        'sub18', 'sub19', 'sub20', 'sub21', 'sub22', 'sub23', 'sub24', 'sub25', 'sub26', 'sub27', 'sub28', 'sub29', \n",
    "       'sub30', 'sub31', 'sub32', 'sub33', 'sub34', 'sub35', 'sub36', 'sub37', 'sub38', 'sub39', 'sub40', 'sub41', \n",
    "       'sub42', 'sub43', 'sub44', 'sub45', 'sub46', 'sub47', 'sub48', 'sub49', 'sub50', 'sub51', 'sub52', 'sub53', \n",
    "       'sub54', 'sub55', 'sub56', 'sub57', 'sub58', 'sub59', 'sub60', 'sub61', 'sub62', 'sub63', 'sub64', 'sub65', 'sub66', \n",
    "       'sub67', 'sub68', 'sub69', 'sub70',  'sub72']\n",
    "\n",
    "#subs = ['sub05']\n",
    "subs_fulldf = []\n",
    "for sub in subs: \n",
    "    subnum = sub[3:]\n",
    "    subnum = int(subnum)\n",
    "    if subnum != 69:\n",
    "        path_sub = \"/data/pt_02917/data/{s}/main_exp/{s}_mainexp.csv\".format(s = sub)\n",
    "\n",
    "    elif subnum == 69:\n",
    "        path_sub = \"/data/pt_02917/data/sub69/main_exp/sub69_complete.csv\"\n",
    "\n",
    "    df = pd.read_csv(path_sub, sep = \";\")\n",
    "\n",
    "    if subnum == 41:\n",
    "        df['subject'] = \"sub41\"\n",
    "    elif subnum == 67:\n",
    "        df['subject'] = \"sub67\"\n",
    "    \n",
    "\n",
    "    ##Create one row for every word \n",
    "    df = df.assign(sentence=df['sentence'].str.split(' ')).explode('sentence')\n",
    "    #Create a column for the word order (important for merging) \n",
    "    w_order = []\n",
    "    for sentence_id in df['target'].unique():\n",
    "        sent_w = df[df['target']== sentence_id]\n",
    "        wo = 1\n",
    "        \n",
    "        for i,r in sent_w.iterrows():\n",
    "            w_order.append(wo)\n",
    "            wo = wo + 1\n",
    "    \n",
    "    df['word_order'] = w_order\n",
    "    ##Make sure that all letters are correctly represented\n",
    "    df['sentence'] = df['sentence'].str.replace(\"Ãľ\", \"Ü\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"Ã¼\", \"ü\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"Ã¶\", \"ö\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"Ã¤\", \"ä\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"ÃŁ\", \"ß\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"ÃĦ\", \"Ä\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"Ãĸ\", \"Ö\")\n",
    "    #Remove Punctuation\n",
    "    df['sentence'] = df['sentence'].str.replace(\",\", \"\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\".\", \"\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\":\", \"\")\n",
    "    df['sentence'] = df['sentence'].str.replace('\"', \"\")\n",
    "    df['sentence'] = df['sentence'].str.replace('!', \"\")\n",
    "    \n",
    "   # df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','', regex=True)\n",
    "\n",
    "\n",
    "    #For merging\n",
    "    df['target_id'] = df['target'].str[0:3]\n",
    "    df['target_id'] = pd.to_numeric(df['target_id'])\n",
    "\n",
    "\n",
    "    df_full = pd.merge(df, surprisal_new, left_on = [ 'target_id', 'word_order'], right_on = [ 'id', 'word_order'], suffixes=('', '_y'))\n",
    "\n",
    "    \n",
    "    df_full.drop(['id', 'Unnamed: 0', 'words'], axis=1, inplace=True)\n",
    "    df_full['words'] = df_full['sentence']\n",
    "    df_full.drop(['sentence'], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    df_full1 = pd.merge(df_full, audibility, left_on = [ 'target_id', 'word_order'], right_on = ['ids',  'word_order'])\n",
    "    df_full1.drop(['Unnamed: 0', 'word', 'ids', 'filename'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    df_full2 = pd.merge(df_full1, word_frequency, left_on = ['target_id', 'word_order'], right_on = ['id',  'word_order'])\n",
    "    ##Checking accuracy = Which words from the actual sentences are in the response\n",
    "\n",
    "    acc = []\n",
    "    misheard = []\n",
    "    start_time = []\n",
    "    end_time = []\n",
    "\n",
    "    for target_id in df_full2['target_id'].unique():\n",
    "        sent_df = df_full2.loc[df_full1['target_id'] == target_id]\n",
    "        sub = sent_df.iloc[0,0]\n",
    "    \n",
    "        \n",
    "        sub_responses = sub_result[sub_result['subject'] == sub]\n",
    "        trial_resp = sub_responses[sub_responses['target_id'] == target_id]\n",
    "\n",
    "        \n",
    "        last_double_word = []\n",
    "\n",
    "        if len(trial_resp) > 0:\n",
    "            \n",
    "            for i1,r1 in trial_resp.iterrows():\n",
    "                resp = r1['resp']\n",
    "        \n",
    "        else:\n",
    "            resp = \"\"\n",
    "\n",
    "        resp = resp.lower()\n",
    "\n",
    "        if resp == \"nicht verstanden\" or resp == \"nichts verstanden\":\n",
    "            resp = \"\"\n",
    "\n",
    "\n",
    "        resp = resp.replace('portraits', 'portäts')\n",
    "        resp = resp.replace('brötchen backen', 'brötchenbacken')\n",
    "        resp = resp.replace('skilaufen', 'ski laufen')\n",
    "        resp = resp.replace('lös', 'löst')\n",
    "        resp = resp.replace('einst weilen', 'einstweilen')\n",
    "        resp = resp.replace('asthma-anfälle', 'asthmaanfälle')\n",
    "        resp = resp.replace('12', 'zwölf')\n",
    "        resp = resp.replace('umgehungsstrasse', 'umgehungsstraße')\n",
    "        resp = resp.replace('30', 'dreißig')\n",
    "        resp = resp.replace('heuaufen', 'heuhaufen')\n",
    "        resp = resp.replace('8', 'acht')\n",
    "        resp = resp.replace('2', 'zwei')\n",
    "        resp = resp.replace('5', 'fünf')\n",
    "        resp = resp.replace('flug', 'pflug')\n",
    "        resp = resp.replace('1000', 'tausend')\n",
    "\n",
    "        resp = resp.replace('gibt es', 'gibts')\n",
    "        resp = resp.replace('über dem', 'überm')\n",
    "        resp = resp.replace('Rauhreif', 'Raureif')\n",
    "        \n",
    "        resp = resp.replace('gewehr', 'gewähr')\n",
    "        resp = resp.replace('oberleutnants', 'oberstleutnants')\n",
    "        resp = resp.replace('bergs', 'berges')\n",
    "        resp = resp.replace('bergesteiger', 'bergsteiger')\n",
    "\n",
    "        resp = re.sub(r'[^\\w\\s]','', resp) \n",
    "        resp_w = resp.split(\" \")\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        for i,r in sent_df.iterrows():\n",
    "            r['words'] = r['words'].lower()\n",
    "            r['words'] = re.sub(r'[^\\w\\s]','', r['words']) \n",
    "    \n",
    "\n",
    "            acc_t = any(r['words'] == w for w in resp_w)\n",
    "\n",
    "            \n",
    "            acc.append(acc_t)\n",
    "       \n",
    "\n",
    "\n",
    "    df_full2['acc'] = acc\n",
    "\n",
    "    df_full2['surprisal']=np.where(df_full2['word_order'] < 2, np.nan, df_full2['surprisal'])\n",
    "    df_full2['frequency']=np.where(df_full2['word_order'] < 2, np.nan, df_full2['frequency'])\n",
    "    df_full2['entropy']=np.where(df_full2['word_order'] < 2, np.nan, df_full2['entropy'])\n",
    "\n",
    "    df_phone = pd.merge(df_full2, audibility_phone, left_on = ['words', 'target_id', 'word_order'], right_on = ['words', 'ids', 'wordnum'])\n",
    "        \n",
    "    df_phone['audibility_word'] = df_phone['audibility_x']\n",
    "    df_phone['audibility_phone'] = df_phone['audibility_y']\n",
    "\n",
    "    df_phone.drop([ 'ids',  'filename', 'audibility_x', 'audibility_y'], axis=1, inplace=True)\n",
    "\n",
    "    df_phone['words'] = df_phone['words'].str.lower()\n",
    "   \n",
    "    df_phone['phone_num']=df_phone.groupby(['words', 'target_id', 'word_order']).cumcount()+1\n",
    "\n",
    "    df_full = pd.merge(df_phone, surprisal_phone, left_on = ['phone', 'words','target_id', 'phone_num'], right_on = ['phone','word', 'target_id', 'phone_num'])\n",
    "\n",
    "    ##For saving (behav analysis)\n",
    "    subs_fulldf.append(df_full2)\n",
    "\n",
    "\n",
    "    ## TRF Feature creation (upsampling) ######\n",
    "    ###Standardize the features\n",
    "\n",
    "\n",
    "    \n",
    "    df_full['phone_surprisal'] = min_max_scaler.fit_transform(df_full[['phone_surprisal']])\n",
    "    df_full['phone_entropy'] = min_max_scaler.fit_transform(df_full[['phone_entropy']])\n",
    "    df_full['surprisal'] = min_max_scaler.fit_transform(df_full[['surprisal']])    \n",
    "    df_full['frequency'] = min_max_scaler.fit_transform(df_full[['frequency']])\n",
    "    df_full['entropy'] = min_max_scaler.fit_transform(df_full[['entropy']])\n",
    "\n",
    "    df_full['surprisal']=np.where(df_full['wordnum'] < 2, np.nan, df_full['surprisal'])\n",
    "    df_full['frequency']=np.where(df_full['wordnum'] < 2, np.nan, df_full['frequency'])\n",
    "    df_full['entropy']=np.where(df_full['wordnum'] < 2, np.nan, df_full['entropy'])\n",
    "\n",
    "    ###Correct and incorrect ids\n",
    "    id_cor = []\n",
    "    id_f = []\n",
    "    idb = []\n",
    "\n",
    "\n",
    "    df_full_nb = df_full.copy()\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_full_con = pd.concat(subs_fulldf)\n",
    "df_full_con.to_csv(\"../statistics/behavdat.csv\", sep = \";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b93a3092-6724-4fa1-9863-b37fedc47792",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['surprisal', 'frequency','entropy', 'phone_surprisal', 'phone_entropy', 'word_onset', 'phone_onset']\n",
    "\n",
    "for feat in feats:\n",
    "    upsamp = []\n",
    "\n",
    "        \n",
    "    for t,dfsent in df_full.groupby('target', sort = False):\n",
    "        \n",
    "        # Parameters\n",
    "        sampling_rate = 128  # Hz\n",
    "        duration = dfsent['end_s_x'].max() # add 1 sec buffer (or use known total duration)\n",
    "        n_samples = int(np.ceil(duration * sampling_rate))\n",
    "        \n",
    "        # Initialize array with zeros\n",
    "        signal = np.zeros(n_samples)\n",
    "\n",
    "        if 'phone' in feat:\n",
    "            onset_samples = (dfsent['start_s_y'] * sampling_rate).round().astype(int)\n",
    "        else:\n",
    "            # Convert onset times to sample indices\n",
    "            onset_samples = (dfsent['start_s_x'] * sampling_rate).round().astype(int)\n",
    "\n",
    "     \n",
    "        \n",
    "        if 'onset' in feat:\n",
    "            signal[onset_samples] = 1\n",
    "        else:\n",
    "        \n",
    "            # Insert values into the signal at the onset sample indices\n",
    "            signal[onset_samples] = dfsent[feat].values\n",
    "        signal = np.nan_to_num(signal)\n",
    "    \n",
    "        upsamp.append(signal)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        with open('./trf_input/{f}_target_final_yo.pickle'.format(f = feat), 'wb') as o:\n",
    "            pickle.dump(upsamp, o)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b2a261b-ab2f-4c24-899f-f1d4f5f37314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save stimulus numbers\n",
    "stimnum = df_full['target_id'].unique().tolist()\n",
    "with open('./trf_input/stimnum_target.pickle', 'wb') as o:\n",
    "    pickle.dump(stimnum, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256021f-b1e2-4ad1-9ef4-02af81affa97",
   "metadata": {},
   "source": [
    "## Distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cef89d5-5a3b-4610-b8db-0a44327a887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "word_surprisal = pd.read_csv(\"./trf_input/word_surprisal_gpt_dis.csv\")\n",
    "#\n",
    "phone_surprisal = pd.read_csv(\"./trf_input/phone_surprisal_entropy_final_dis.csv\")\n",
    "\n",
    "directory = './word_seg_distractors/'\n",
    "\n",
    "sent_id = []\n",
    "words = []\n",
    "start = []\n",
    "end = []\n",
    "word_order = []\n",
    "\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            idx = file[:-4]\n",
    "            p = os.path.join(directory, file)\n",
    "            f=pd.read_csv(p, sep = \";\")\n",
    "            \n",
    "            ##Remove silence in the beginning and end\n",
    "            f = f.drop(f.loc[(f.MAU == \"(...)\")].index)\n",
    "            \n",
    "            f = f.drop(f.loc[(f.MAU.isnull())].index)\n",
    "\n",
    "            f = f.drop(f.loc[f.TOKEN == -1].index)\n",
    "    \n",
    "            words_sent = list(f['TOKEN'].unique())\n",
    "            wo = 1\n",
    "            \n",
    "            for w in words_sent:\n",
    "                f_w = f.loc[f['TOKEN'] == w]\n",
    "                word_start = f_w.iloc[0,0]\n",
    "                word_end = f_w.iloc[-1,0] + f_w.iloc[-1,1]\n",
    "                word = f_w.iloc[0,5]\n",
    "    \n",
    "                word_order.append(wo)\n",
    "                wo = wo + 1\n",
    "                \n",
    "                sent_id.append(idx)\n",
    "                #print(idx)\n",
    "                words.append(word)\n",
    "                start.append(word_start)\n",
    "                end.append(word_end)\n",
    "\n",
    "sent_words = pd.DataFrame(zip(sent_id, words, start, end, word_order), columns = ['id', 'word', 'start', 'end', 'word_order'])\n",
    "\n",
    "##Calculate seconds\n",
    "sent_words['start_s'] = sent_words['start']/44100\n",
    "sent_words['end_s'] = sent_words['end']/44100\n",
    "\n",
    "\n",
    "merge1 = pd.merge(sent_words, word_surprisal, on = ['id', 'word_order'])\n",
    "merge1.drop(['words', 'Unnamed: 0'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "targets_complete = pd.read_csv('./target_distractor_final_final.csv', sep = \";\")\n",
    "\n",
    "directory = './word_seg_distractors/'\n",
    "\n",
    "\n",
    "sent_id = []\n",
    "words = []\n",
    "phones = []\n",
    "start = []\n",
    "end = []\n",
    "word_order = []\n",
    "wordnum = []\n",
    "syl_nums = []\n",
    "syls = []\n",
    "\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file[:7] in list(targets_complete['distractor'].str[:7]):\n",
    "\n",
    "            p = os.path.join(directory, file)\n",
    "            if p.endswith(\".csv\"):\n",
    "                f=pd.read_csv(p, sep = \";\")\n",
    "            \n",
    "                ##Remove silence in the beginning and end\n",
    "                f = f.drop(f.loc[(f.MAU == \"(...)\")].index)\n",
    "                f = f.drop(f.loc[f.TOKEN == -1].index)\n",
    "                f = f.drop(f.loc[(f.MAU.isnull())].index)\n",
    "\n",
    "                f = f.reset_index(drop=True)\n",
    "                wo = 1\n",
    "        \n",
    "                syl_add = \"\"\n",
    "                syl_num = 1\n",
    "                syl_count = 1\n",
    "                \n",
    "                for i,p1 in f.iterrows():\n",
    "                    f_w = p1\n",
    "                    phone_start = p1.iloc[0]\n",
    "                    phone_end = p1.iloc[0] + p1.iloc[1]\n",
    "                    phone = p1.iloc[3]\n",
    "                    word = p1.iloc[5]\n",
    "                    numw = int(p1.iloc[2])+1\n",
    "        \n",
    "                    word_order.append(wo)\n",
    "                    wo = wo + 1\n",
    "        \n",
    "    \n",
    "        \n",
    "                    \n",
    "                    sent_id.append(file[:7])\n",
    "                    phones.append(phone)\n",
    "                    words.append(word)\n",
    "                    wordnum.append(numw)\n",
    "        \n",
    "                    start.append(phone_start)\n",
    "                    end.append(phone_end)\n",
    "                \n",
    "\n",
    "\n",
    "sent_words = pd.DataFrame(zip(sent_id, words, phones, start, end, word_order, wordnum), columns = ['id', 'words', 'phone', 'start', 'end', 'phone_order', 'wordnum'])\n",
    "\n",
    "##Calculate seconds\n",
    "sent_words['start_s'] = sent_words['start']/44100\n",
    "sent_words['end_s'] = sent_words['end']/44100\n",
    "\n",
    "sent_words['phone_num'] = sent_words.groupby(['id', 'words', 'wordnum']).cumcount()+1\n",
    "\n",
    "\n",
    "merge2 = pd.merge(sent_words, phone_surprisal, left_on = ['id', 'wordnum', 'phone_num'], right_on = ['id', 'word_order', 'phone_num'])\n",
    "merge2.drop(['words', 'Unnamed: 0',  'wordnum', 'phone_order'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "merge_surpdis = pd.merge(merge1, merge2, on = ['id', 'word_order'])\n",
    "\n",
    "freq_dis = pd.read_csv(\"./trf_input/word_frequency_dis_lgsubtlex.csv\")\n",
    "\n",
    "\n",
    "merge_surpdis_freq = pd.merge(merge_surpdis, freq_dis, on = ['id', 'word_order'])\n",
    "\n",
    "merge_surpdis_freq['surprisal']=np.where(merge_surpdis_freq['word_order'] == 1, np.nan, merge_surpdis_freq['surprisal'])\n",
    "merge_surpdis_freq['frequency']=np.where(merge_surpdis_freq['word_order'] == 1, np.nan, merge_surpdis_freq['frequency'])\n",
    "merge_surpdis_freq['entropy']=np.where(merge_surpdis_freq['word_order'] == 1, np.nan, merge_surpdis_freq['entropy'])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "merge_surpdis_freq['surprisal'] = min_max_scaler.fit_transform(merge_surpdis_freq[['surprisal']])\n",
    "merge_surpdis_freq['phone_surprisal'] = min_max_scaler.fit_transform(merge_surpdis_freq[['phone_surprisal']])\n",
    "merge_surpdis_freq['phone_entropy'] = min_max_scaler.fit_transform(merge_surpdis_freq[['phone_entropy']])\n",
    "merge_surpdis_freq['frequency'] = min_max_scaler.fit_transform(merge_surpdis_freq[['frequency']])\n",
    "merge_surpdis_freq['entropy']=min_max_scaler.fit_transform(merge_surpdis_freq[['entropy']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53ed1fae-6df1-4a3c-9618-a36510d03279",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['surprisal', 'frequency','entropy', 'phone_surprisal', 'phone_entropy', 'word_onset', 'phone_onset']\n",
    "\n",
    "for feat in feats:\n",
    "    upsamp = []\n",
    "\n",
    "        \n",
    "    for t,dfsent in merge_surpdis_freq.groupby('id', sort = False):\n",
    "        \n",
    "        # Parameters\n",
    "        sampling_rate = 128  # Hz\n",
    "        duration = dfsent['end_s_y'].max()   \n",
    "        n_samples = int(np.ceil(duration * sampling_rate))\n",
    "        \n",
    "        # Initialize array with zeros\n",
    "        signal = np.zeros(n_samples)\n",
    "\n",
    "        if 'phone' in feat:\n",
    "            onset_samples = (dfsent['start_s_y'] * sampling_rate).round().astype(int)\n",
    "        else:\n",
    "            # Convert onset times to sample indices\n",
    "            onset_samples = (dfsent['start_s_x'] * sampling_rate).round().astype(int)\n",
    "\n",
    "        \n",
    "        if 'onset' in feat:\n",
    "            signal[onset_samples] = 1\n",
    "        else:\n",
    "        \n",
    "            # Insert values into the signal at the onset sample indices\n",
    "            signal[onset_samples] = dfsent[feat].values\n",
    "        signal = np.nan_to_num(signal)\n",
    "    \n",
    "        upsamp.append(signal)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        with open('./trf_input/{f}_dis_final_yo.pickle'.format(f = feat), 'wb') as o:\n",
    "            pickle.dump(upsamp, o)\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66db3142-9067-40f2-83ca-1cb4302a0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save stimulus numbers\n",
    "stimnum = merge_surpdis_freq['id'].unique().tolist()\n",
    "with open('./trf_input/stimnum_dis.pickle', 'wb') as o:\n",
    "    pickle.dump(stimnum, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ba33b-64ea-4827-b979-14c0169481ac",
   "metadata": {},
   "source": [
    "## Envelopes, derivative, and f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c6ee5d5-5aaf-49bb-992c-3afea42e9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, find_peaks, resample\n",
    "import librosa \n",
    "import naplib\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "logging.getLogger(\"naplib\").setLevel(logging.FATAL)\n",
    "\n",
    "sub='sub05'\n",
    "\n",
    "#Load behavioral log file\n",
    "path_sub = \"/data/pt_02917/data/{s}/main_exp/{s}_mainexp.csv\".format(s = sub)\n",
    "df = pd.read_csv(path_sub, sep = \";\")\n",
    "\n",
    "target_path = \"../behavior_stimuli/targets_final_norm/\"\n",
    "distractor_path = \"../behavior_stimuli/distractors_new_clean/\"\n",
    "\n",
    "envelopes_target = []\n",
    "rates_target = []\n",
    "target_f0 = []\n",
    "\n",
    "envelopes_dis = []\n",
    "rates_dis = []\n",
    "dis_f0 = []\n",
    "stimnum = []\n",
    "\n",
    "from scipy.stats import zscore\n",
    "for i,r in df.iterrows():\n",
    "    num = int(r['target'][0:3])\n",
    "    stimnum.append(num)\n",
    "\n",
    "    target, sr = librosa.load(target_path + r['target'], sr = 44100)\n",
    "    distractor, sr = librosa.load(distractor_path + r['distractor'], sr = 44100)\n",
    "    \n",
    "\n",
    "   # zero pad the target\n",
    "    zer = np.array([0]*22050)\n",
    "    y1_sil = np.concatenate((zer,target))\n",
    "    \n",
    "    zer2 = np.array([0]*(len(distractor)-len(y1_sil)))\n",
    "    target_pad = np.concatenate((y1_sil, zer2))\n",
    "\n",
    "    #####Target    \n",
    "    ###Calculate the spectrogram \n",
    "    spec= naplib.features.auditory_spectrogram(target_pad, sr)\n",
    "\n",
    "    # Extract the envelope by taking the mean over frequencies\n",
    "    env = np.mean(spec, axis=1)\n",
    "    env = scipy.signal.resample(env, round(len(env)/125*128))    \n",
    "   \n",
    "    # Temporal derivative \n",
    "    rate = np.maximum(np.diff(env, prepend=env[0]), 0)\n",
    "    rate = scipy.signal.resample(rate, len(env))\n",
    "\n",
    "    #F0\n",
    "    f0t,v,vp= librosa.pyin(target_pad,sr=sr,fmin=75, fmax=175)\n",
    "    f0t_p = np.nan_to_num(f0t, nan = np.nanmean(f0t))\n",
    "    f0t_p = scipy.signal.resample(f0t_p, len(env))\n",
    "\n",
    "    env_target = min_max_scaler.fit_transform(env.reshape(-1,1))\n",
    "    rate_target = min_max_scaler.fit_transform(rate.reshape(-1,1))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ###Same for the distractor\n",
    "    spec= naplib.features.auditory_spectrogram(distractor, sr)\n",
    "\n",
    "    # Extract the envelope by taking the mean over frequencies\n",
    "    env = np.mean(spec, axis=1)\n",
    "    env = scipy.signal.resample(env, round(len(env)/125*128))    \n",
    "  \n",
    "    # Temporal derivative \n",
    "    rate = np.maximum(np.diff(env, prepend=env[0]), 0)\n",
    "    rate = scipy.signal.resample(rate, len(env))\n",
    "\n",
    "    ##F0 \n",
    "    f0d,v,vp = librosa.pyin(distractor,sr=sr,fmin=120, fmax=300)\n",
    "    f0d_p = np.nan_to_num(f0d, nan = np.nanmean(f0d))   \n",
    "    f0d_p = scipy.signal.resample(f0d_p, len(env))\n",
    "\n",
    "\n",
    "    env = min_max_scaler.fit_transform(env.reshape(-1,1))\n",
    "    rate = min_max_scaler.fit_transform(rate.reshape(-1,1))\n",
    "\n",
    "\n",
    "    zer = np.array([0]*64)\n",
    "    env_target = env_target.reshape(env_target.shape[0])\n",
    "    rate_target = rate_target.reshape(env_target.shape[0])\n",
    "    \n",
    "  \n",
    "    \n",
    "    rates_target.append(rate_target)\n",
    "    envelopes_target.append(env_target)\n",
    "    target_f0.append(f0t_p)\n",
    "\n",
    "    \n",
    "    rates_dis.append(rate)\n",
    "    envelopes_dis.append(env)\n",
    "    dis_f0.append(f0d_p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "with open(\"./trf_input/envelope_target.pickle\".format(sub = sub), \"wb\") as output_file:\n",
    "    pickle.dump(envelopes_target, output_file)\n",
    "\n",
    "with open(\"./trf_input/envelope_dis.pickle\".format(sub = sub), \"wb\") as output_file:\n",
    "    pickle.dump(envelopes_dis, output_file)\n",
    "\n",
    "\n",
    "with open(\"./trf_input/rate_target.pickle\".format(sub = sub), \"wb\") as output_file:\n",
    "    pickle.dump(rates_target, output_file)\n",
    "\n",
    "with open(\"./trf_input/rate_dis.pickle\".format(sub = sub), \"wb\") as output_file:\n",
    "    pickle.dump(rates_dis, output_file)\n",
    "\n",
    "\n",
    "with open(\"./trf_input/stimnum_env.pickle\".format(sub = sub), \"wb\") as output_file:\n",
    "    pickle.dump(stimnum, output_file)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "##Standardize F0 \n",
    "all_f0_t = np.concatenate(target_f0)\n",
    "all_f0_d = np.concatenate(dis_f0)\n",
    "\n",
    "all_f0 = np.concatenate([all_f0_t, all_f0_d])\n",
    "\n",
    "mean_f0 = np.mean(all_f0)\n",
    "std_f0 = np.std(all_f0)\n",
    "\n",
    "f0_target_stan = []\n",
    "f0_dis_stan = []\n",
    "for i,f in enumerate(target_f0):\n",
    "    target_f0_stan = (f-mean_f0)/std_f0\n",
    "    dis_f0_stan = (dis_f0[i]-mean_f0)/std_f0\n",
    "    f0_target_stan.append(target_f0_stan)\n",
    "    f0_dis_stan.append(dis_f0_stan)\n",
    "        \n",
    "\n",
    "\n",
    "with open(\"./trf_input/f0_target.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(f0_target_stan, output_file)\n",
    "\n",
    "with open(\"./trf_input/f0_dis.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(f0_dis_stan, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bd63d44-8731-4a10-92ea-83cb857ac1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject                                                            sub05\n",
       "trialnum                                                             240\n",
       "gender_follow                                                       male\n",
       "target                                                           004.wav\n",
       "distractor                                                   dis_070.wav\n",
       "sentence                             Das kalte Licht leuchtet am Morgen.\n",
       "distractor_sentence    Hinter den Augen verbirgt sich die Magie des U...\n",
       "srt_db                                                        -13.661274\n",
       "srt_snr                                                        -6.830637\n",
       "db_cond                                                               -2\n",
       "trial_start                                                  11920.20497\n",
       "sound_stop                                                  11924.154364\n",
       "resp                                                                   1\n",
       "respt                                                           3.106774\n",
       "name_audio             C:\\Documents and Settings\\presentation\\Desktop...\n",
       "Name: 239, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f951cb-84a2-4e50-bc6e-0641168ee05d",
   "metadata": {},
   "source": [
    "## Speaker features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97f254cc-af6e-4cfb-a66c-4b0d3a62b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat = pd.read_csv(\"./ratings.csv\")\n",
    "\n",
    "vertrauen = rat.iloc[1]['vertrauen.response']\n",
    "age = rat.iloc[1]['age.response']\n",
    "gebildet = rat.iloc[1]['gebildet.response']\n",
    "\n",
    "dominant = rat.iloc[1]['Dominant.response']\n",
    "attraktiv = rat.iloc[1]['Attraktiv.response']\n",
    "gesund=rat.iloc[1]['gesund.response']\n",
    "professionell = rat.iloc[1]['professionell.response']\n",
    "\n",
    "dict_rat = {'vertrauen':vertrauen, 'age':age,'gebildet':gebildet, 'dominant':dominant, 'attraktiv': attraktiv, \n",
    "            'gesund':gesund,'professionell':professionell}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a9af005-c348-4e2a-add6-b6b6c5da1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['vertrauen', 'age','gebildet', 'dominant', 'attraktiv', 'gesund', 'professionell']\n",
    "\n",
    "for feat in feats:\n",
    "    upsamp = []\n",
    "\n",
    "        \n",
    "    for t,dfsent in df_full.groupby('target', sort = False):\n",
    "        \n",
    "        # Parameters\n",
    "        sampling_rate = 128  # Hz\n",
    "        duration = dfsent['end_s_y'].max()   \n",
    "        n_samples = int(np.ceil(duration * sampling_rate))\n",
    "        \n",
    "        # Initialize array with zeros\n",
    "        signal = np.zeros(n_samples)\n",
    "\n",
    "        \n",
    "        # Convert onset times to sample indices\n",
    "        onset_samples = (dfsent['start_s_x'] * sampling_rate).round().astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "        # Insert values into the signal at the onset sample indices\n",
    "        signal[onset_samples] = dict_rat[feat]\n",
    "        \n",
    "        signal = np.nan_to_num(signal)\n",
    "    \n",
    "        upsamp.append(signal)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        with open('./trf_input/{f}_target.pickle'.format(f = feat), 'wb') as o:\n",
    "            pickle.dump(upsamp, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "297e1c2a-5bbe-4581-82cf-5ca24d1823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertrauen = rat.iloc[0]['vertrauen.response']\n",
    "age = rat.iloc[0]['age.response']\n",
    "gebildet = rat.iloc[0]['gebildet.response']\n",
    "\n",
    "dominant = rat.iloc[0]['Dominant.response']\n",
    "attraktiv = rat.iloc[0]['Attraktiv.response']\n",
    "gesund = rat.iloc[0]['gesund.response']\n",
    "professionell = rat.iloc[0]['professionell.response']\n",
    "\n",
    "dict_rat = {'vertrauen':vertrauen, 'age':age,'gebildet':gebildet, 'dominant':dominant, 'attraktiv': attraktiv, \n",
    "            'gesund':gesund,'professionell':professionell}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "029e8b26-f14e-4b83-8113-936b145f1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['vertrauen', 'age','gebildet', 'dominant', 'attraktiv', 'gesund', 'professionell']\n",
    "\n",
    "for feat in feats:\n",
    "    upsamp = []\n",
    "\n",
    "        \n",
    "    for t,dfsent in merge_surpdis_freq.groupby('id', sort = False):\n",
    "        \n",
    "        # Parameters\n",
    "        sampling_rate = 128  # Hz\n",
    "        duration = dfsent['end_s_y'].max()   \n",
    "        n_samples = int(np.ceil(duration * sampling_rate))\n",
    "        \n",
    "        # Initialize array with zeros\n",
    "        signal = np.zeros(n_samples)\n",
    "\n",
    "        \n",
    "        # Convert onset times to sample indices\n",
    "        onset_samples = (dfsent['start_s_x'] * sampling_rate).round().astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "        # Insert values into the signal at the onset sample indices\n",
    "        signal[onset_samples] = dict_rat[feat]\n",
    "        \n",
    "        signal = np.nan_to_num(signal)\n",
    "    \n",
    "        upsamp.append(signal)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        with open('./trf_input/{f}_dis.pickle'.format(f = feat), 'wb') as o:\n",
    "            pickle.dump(upsamp, o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

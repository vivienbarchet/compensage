{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3bbb17-7898-4dd8-b4c7-4f3cdd9e2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd2ed8c-5865-49d0-86f1-c0f1243264e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load word frequencies\n",
    "subtlex = pd.read_excel(\"/data/tu_barchet_cloud/owncloud-gwdg/data_analysis/phoneme_surprisal/SUBTLEX-DE cleaned with Google00 frequencies.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07589ce2-6ad6-470a-b940-21f52ed8e1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asthmaanfälle\n",
      "peitschenknall\n",
      "amtsblatt\n",
      "blumenkasten\n",
      "brötchenbacken\n",
      "feldscheune\n",
      "deutschlehrer\n",
      "letztmalige\n",
      "kinderfahrräder\n",
      "lichtbild\n",
      "kaffeeplausch\n",
      "studienkurs\n",
      "hilfsmannschaft\n",
      "eckfenster\n",
      "abendnebel\n",
      "keimte\n",
      "küchenofen\n",
      "sandwüste\n",
      "flirrt\n",
      "linkslastigen\n",
      "plätschernden\n",
      "altersstufe\n",
      "wirtschaftsbank\n",
      "raureif\n",
      "verkehrsampeln\n"
     ]
    }
   ],
   "source": [
    "subtlex = subtlex.dropna(subset = \"Word\")\n",
    "subtlex = subtlex[['Word', 'SUBTLEX']]\n",
    "subtlex['Word'] = subtlex['Word'].str.lower()\n",
    "\n",
    "#Load my sentences\n",
    "target_list = pd.read_csv('./target_distractor_final_final.csv', sep = \";\")\n",
    "\n",
    "sentences = target_list['target_sent']\n",
    "\n",
    "\n",
    "new_words = []\n",
    "freqs_new = []\n",
    "for s in sentences:\n",
    "    wordlist = s.split(\" \")\n",
    "\n",
    "    for w in wordlist:\n",
    "        w = w.replace(\",\", \"\")\n",
    "        w = w.replace(',', '')\n",
    "        w = w.replace(':', '')\n",
    "        w = w.replace('!', '')\n",
    "        w = w.replace('\"', '')\n",
    "        w = w.replace('.', '')\n",
    "\n",
    "        w = w.lower()\n",
    "        f_p = subtlex[subtlex['Word'] == w]\n",
    "\n",
    "        if len(f_p) == 0:\n",
    "            w = w.replace(\"ß\", \"ss\")\n",
    "            f_p = subtlex[subtlex['Word'] == w]\n",
    "\n",
    "        if len(f_p) == 0:\n",
    "            print(w)\n",
    "            new_words.append(w)\n",
    "            freqs_new.append(min(subtlex['SUBTLEX']))\n",
    "            \n",
    "\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0b1933-4789-4ce7-b022-ba3b626e96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add words to subtlex\n",
    "add_words = pd.DataFrame(zip(new_words, freqs_new), columns = ['Word', 'SUBTLEX'])\n",
    "subtlex_complete = pd.concat([subtlex, add_words])\n",
    "\n",
    "nw = subtlex_complete['SUBTLEX'].sum()\n",
    "\n",
    "subtlex_complete = subtlex_complete.dropna(subset = \"Word\")\n",
    "subtlex_complete['SUBTLEX'] = subtlex_complete['SUBTLEX']/nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d462a2d8-015e-4806-aa9b-119ae43c06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text files for phonemic transcription\n",
    "wordnum = 1\n",
    "\n",
    "for chunk in range(1,21):\n",
    "    fname = 'allw' + str(chunk) + '.txt'\n",
    "    fp= open(fname, 'w', encoding = \"UTF-8\")\n",
    "\n",
    "    if chunk == 1:\n",
    "        freqs = subtlex_complete[0:20000]\n",
    "    elif chunk == 20:\n",
    "        freqs = subtlex_complete[(chunk-1)*20000+1:]\n",
    "    else:\n",
    "        freqs = subtlex_complete[(chunk-1)*20000+1:(chunk*20000)]\n",
    "    \n",
    "\n",
    "    for i,r in freqs.iterrows():\n",
    "            \n",
    "        if r['Word']== True:\n",
    "            w = \"wahr\"\n",
    "        else:\n",
    "            w = r['Word']\n",
    "    \n",
    "        \n",
    "        fp.write(w+ \"\\n\")\n",
    "    \n",
    "    \n",
    "    fp.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551c99ba-9235-42eb-a121-f5ac7fd462b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import io\n",
    "\n",
    "#Load outputs from webmaus\n",
    "root = './phones_final/'\n",
    "\n",
    "phones_df = []\n",
    "\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if name.startswith('allw') and name.endswith('.par'):\n",
    "            wordnum = []\n",
    "            phones = []\n",
    "            words = []\n",
    "            countp = 0\n",
    "            countw = 0\n",
    "            \n",
    "            file = os.path.join(path, name)\n",
    "                    \n",
    "            f = io.open(file, mode = \"r\", encoding = \"utf-8\") \n",
    "            lines = f.readlines()           \n",
    "\n",
    "            for l in lines:\n",
    "                if l.startswith(\"KAN\"):\n",
    "                    if countp < 10:\n",
    "                        n = l[7:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    elif countp >=10 and countp < 100:\n",
    "                        n = l[8:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    elif countp >= 100 and countp < 1000:\n",
    "                        n = l[9:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    elif countp >= 1000 and countp < 10000:\n",
    "                        n = l[10:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    else:\n",
    "                        n = l[11:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                \n",
    "                    wordnum.append(countp)\n",
    "                    phones.append(n)\n",
    "                    countp = countp + 1\n",
    "                \n",
    "                else:\n",
    "                    if countw < 10:\n",
    "                        n = l[7:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    elif countw >=10 and countw < 100:\n",
    "                        n = l[8:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    elif countw >= 100 and countw < 1000:\n",
    "                        n = l[9:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    elif countw >= 1000 and countw< 10000:\n",
    "                        n = l[10:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                    else:\n",
    "                        n = l[11:]\n",
    "                        n = n.replace(\"\\n\", \"\")\n",
    "                \n",
    "                \n",
    "                    words.append(n)\n",
    "                    countw = countw + 1\n",
    "            df = pd.DataFrame(zip(words, phones), columns = ['words', 'phones'])\n",
    "        \n",
    "            phones_df.append(df)\n",
    "\n",
    "\n",
    "phones_concat = pd.concat(phones_df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a8e2f3-5227-4b38-9951-8926f6acaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex_complete['Word'] = subtlex_complete['Word'].str.lower()\n",
    "freq_phone = pd.merge(subtlex_complete, phones_concat, left_on = \"Word\", right_on = \"words\")\n",
    "\n",
    "freq_phone = freq_phone.drop_duplicates(subset = ['Word'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b972d760-05ed-4a06-8c01-b7429ea44419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"6\", \"ɐ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"I\", \"ɪ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"S\", \"ʃ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"N\", \"ŋ\")\n",
    "\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"@\", \"ə\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"g\", \"ɡ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"E\", \"ɛ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"?\", \"ʔ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"C\", \"ç\")\n",
    "\n",
    "\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"O\", \"ɔ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\":\", \"ː\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"U\", \"ʊ\")\n",
    "\n",
    "\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"ɔY\", \"ɔʏ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"2:\", \"øː\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"Y\", \"ʏ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"Z\", \"ʒ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"9\", \"œ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"dZ\", \"dʒ\")\n",
    "freq_phone['phones'] = freq_phone['phones'].str.replace(\"2ː\", \"øː\") \n",
    "\n",
    "freq_phone['phone_split'] = freq_phone['phones'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12a145c-05bb-4c60-bf77-1f9412a6cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "phones = np.concatenate(list(freq_phone['phone_split']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fda8643-de4e-4058-a6ab-6382df5f2660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "phone_ent = []\n",
    "phone_surp = []\n",
    "phone = []\n",
    "word = []\n",
    "sentence = []\n",
    "target_id = []\n",
    "word_order = []\n",
    "for i,s in enumerate(sentences):\n",
    "    print(i)\n",
    "    count = 1\n",
    "    wordlist = s.split(\" \")\n",
    "\n",
    "    for w in wordlist:\n",
    "        w = w.replace(\",\", \"\")\n",
    "        w = w.replace(',', '')\n",
    "        w = w.replace(':', '')\n",
    "        w = w.replace('!', '')\n",
    "        w = w.replace('\"', '')\n",
    "        w = w.replace('.', '')\n",
    "\n",
    "        w = w.lower()\n",
    "        f_p = freq_phone[freq_phone['Word'] == w]\n",
    "\n",
    "        if len(f_p) == 0:\n",
    "            w = w.replace(\"ß\", \"ss\")\n",
    "            f_p = freq_phone[freq_phone['Word'] == w]\n",
    "\n",
    "\n",
    "        if len(f_p) == 1:\n",
    "\n",
    "\n",
    "            phonenum = 0\n",
    "            prevphone = []\n",
    "            for p in f_p['phone_split'].iloc[0]:\n",
    "                \n",
    "                if phonenum > 0:\n",
    "                    ##Previous phoneme sequence\n",
    "                    phoneseq = ' '.join(prevphone)\n",
    "                    curphone = phoneseq + \" \" + p\n",
    "        \n",
    "                    prev = freq_phone[freq_phone['phones'].str.startswith(phoneseq)]\n",
    "                    cur = freq_phone[freq_phone['phones'].str.startswith(curphone)]\n",
    "\n",
    "                    cur = cur.copy() \n",
    "                    #phoneme entropy\n",
    "                    ent_phone = -np.sum(prev['SUBTLEX']*np.log2(prev['SUBTLEX']))\n",
    "          \n",
    "                    sum_prev = sum(prev['SUBTLEX'])\n",
    "                    sum_cur = sum(cur['SUBTLEX'])\n",
    "\n",
    "                    #phoneme surprisal\n",
    "                    surp_phone = -math.log(sum_cur/sum_prev, 2)\n",
    "        \n",
    "                    phone_surp.append(surp_phone)\n",
    "                    phone_ent.append(ent_phone)\n",
    "                else:\n",
    "                    phone_surp.append(np.nan)\n",
    "                    phone_ent.append(np.nan)\n",
    "                    \n",
    "\n",
    "                prevphone.append(p)\n",
    "                phonenum = phonenum + 1\n",
    "        \n",
    "                phone.append(p)\n",
    "                word.append(w)\n",
    "                sentence.append(s)\n",
    "                target_id.append(target_list.iloc[i,2])\n",
    "                word_order.append(count)\n",
    "                \n",
    "        else:\n",
    "            phone_surp.append(np.nan)\n",
    "            phone.append(np.nan)\n",
    "            word.append(w)\n",
    "            sentence.append(s)\n",
    "            print(w)\n",
    "\n",
    "\n",
    "        count = count + 1\n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3ab90bf-7046-44eb-b754-0dab63f5a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_surprisal = pd.DataFrame(zip(phone, phone_surp,phone_ent, word, target_id, word_order), columns = ['phone', 'phone_surprisal','phone_entropy', 'word', 'target_id', 'word_order'])\n",
    "phone_surprisal['phone_num']=phone_surprisal.groupby(['word', 'target_id', 'word_order']).cumcount()+1\n",
    "\n",
    "phone_surprisal.to_csv('phone_surprisal_entropy_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
